{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings: Semantics of the language\n",
    "\n",
    "We have some sentences and want to compare the semantic distance of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.6.0-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.15.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from sentence-transformers) (0.29.3)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: filelock in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Using cached sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Using cached torch-2.6.0-cp311-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Using cached scipy-1.15.2-cp311-cp311-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: threadpoolctl, sympy, scipy, safetensors, Pillow, networkx, MarkupSafe, joblib, scikit-learn, jinja2, torch, transformers, sentence-transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.1.0 jinja2-3.1.6 joblib-1.4.2 networkx-3.4.2 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence-transformers-3.4.1 sympy-1.13.1 threadpoolctl-3.6.0 torch-2.6.0 transformers-4.49.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'Wie lang muss ein Langbogen sein?',\n",
    "    'Wie lang muss ein Langbogen mindestens sein?',\n",
    "    'Mensch',\n",
    "    'Human',\n",
    "    'Die Hunde spielen mit dem Ball auf der Wiese',\n",
    "    'Das Rudel rollt das runde Spielgerät auf dem Rasen herum',\n",
    "    'The dogs play with the ball on the grass',\n",
    "    'Der Ballon fliegt in den Himmel',\n",
    "    'Die Taube kehrt zum Nest zurück',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create embeddings using Huggingface models\n",
    "\n",
    "Models can, for example, be found at the [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianweyer/Sources/gen-ai-customer-workshop-march-2025/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0218505  -0.01280628 -0.03129406 ... -0.0346019  -0.02073702\n",
      "  0.02748242]\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'intfloat/multilingual-e5-large'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "\n",
    "print(embeddings[0])\n",
    "for embedding in embeddings:\n",
    "    print(len(embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000: Wie lang muss ein Langbogen sein? <-> Wie lang muss ein Langbogen sein?\n",
      "0.9874: Wie lang muss ein Langbogen sein? <-> Wie lang muss ein Langbogen mindestens sein?\n",
      "0.7513: Wie lang muss ein Langbogen sein? <-> Mensch\n",
      "0.7260: Wie lang muss ein Langbogen sein? <-> Human\n",
      "0.7208: Wie lang muss ein Langbogen sein? <-> Die Hunde spielen mit dem Ball auf der Wiese\n",
      "0.7564: Wie lang muss ein Langbogen sein? <-> Das Rudel rollt das runde Spielgerät auf dem Rasen herum\n",
      "0.6436: Wie lang muss ein Langbogen sein? <-> The dogs play with the ball on the grass\n",
      "0.7806: Wie lang muss ein Langbogen sein? <-> Der Ballon fliegt in den Himmel\n",
      "0.7600: Wie lang muss ein Langbogen sein? <-> Die Taube kehrt zum Nest zurück\n",
      "1.0000: Wie lang muss ein Langbogen mindestens sein? <-> Wie lang muss ein Langbogen mindestens sein?\n",
      "0.7475: Wie lang muss ein Langbogen mindestens sein? <-> Mensch\n",
      "0.7197: Wie lang muss ein Langbogen mindestens sein? <-> Human\n",
      "0.7157: Wie lang muss ein Langbogen mindestens sein? <-> Die Hunde spielen mit dem Ball auf der Wiese\n",
      "0.7542: Wie lang muss ein Langbogen mindestens sein? <-> Das Rudel rollt das runde Spielgerät auf dem Rasen herum\n",
      "0.6409: Wie lang muss ein Langbogen mindestens sein? <-> The dogs play with the ball on the grass\n",
      "0.7697: Wie lang muss ein Langbogen mindestens sein? <-> Der Ballon fliegt in den Himmel\n",
      "0.7605: Wie lang muss ein Langbogen mindestens sein? <-> Die Taube kehrt zum Nest zurück\n",
      "1.0000: Mensch <-> Mensch\n",
      "0.8864: Mensch <-> Human\n",
      "0.7006: Mensch <-> Die Hunde spielen mit dem Ball auf der Wiese\n",
      "0.7186: Mensch <-> Das Rudel rollt das runde Spielgerät auf dem Rasen herum\n",
      "0.6541: Mensch <-> The dogs play with the ball on the grass\n",
      "0.7402: Mensch <-> Der Ballon fliegt in den Himmel\n",
      "0.7594: Mensch <-> Die Taube kehrt zum Nest zurück\n",
      "1.0000: Human <-> Human\n",
      "0.6764: Human <-> Die Hunde spielen mit dem Ball auf der Wiese\n",
      "0.7090: Human <-> Das Rudel rollt das runde Spielgerät auf dem Rasen herum\n",
      "0.6709: Human <-> The dogs play with the ball on the grass\n",
      "0.7206: Human <-> Der Ballon fliegt in den Himmel\n",
      "0.7268: Human <-> Die Taube kehrt zum Nest zurück\n",
      "1.0000: Die Hunde spielen mit dem Ball auf der Wiese <-> Die Hunde spielen mit dem Ball auf der Wiese\n",
      "0.8659: Die Hunde spielen mit dem Ball auf der Wiese <-> Das Rudel rollt das runde Spielgerät auf dem Rasen herum\n",
      "0.8993: Die Hunde spielen mit dem Ball auf der Wiese <-> The dogs play with the ball on the grass\n",
      "0.8146: Die Hunde spielen mit dem Ball auf der Wiese <-> Der Ballon fliegt in den Himmel\n",
      "0.7427: Die Hunde spielen mit dem Ball auf der Wiese <-> Die Taube kehrt zum Nest zurück\n",
      "1.0000: Das Rudel rollt das runde Spielgerät auf dem Rasen herum <-> Das Rudel rollt das runde Spielgerät auf dem Rasen herum\n",
      "0.8116: Das Rudel rollt das runde Spielgerät auf dem Rasen herum <-> The dogs play with the ball on the grass\n",
      "0.8299: Das Rudel rollt das runde Spielgerät auf dem Rasen herum <-> Der Ballon fliegt in den Himmel\n",
      "0.7906: Das Rudel rollt das runde Spielgerät auf dem Rasen herum <-> Die Taube kehrt zum Nest zurück\n",
      "1.0000: The dogs play with the ball on the grass <-> The dogs play with the ball on the grass\n",
      "0.7377: The dogs play with the ball on the grass <-> Der Ballon fliegt in den Himmel\n",
      "0.6611: The dogs play with the ball on the grass <-> Die Taube kehrt zum Nest zurück\n",
      "1.0000: Der Ballon fliegt in den Himmel <-> Der Ballon fliegt in den Himmel\n",
      "0.8201: Der Ballon fliegt in den Himmel <-> Die Taube kehrt zum Nest zurück\n",
      "1.0000: Die Taube kehrt zum Nest zurück <-> Die Taube kehrt zum Nest zurück\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(left, right):\n",
    "    return np.dot(left, right) / (np.linalg.norm(left) * np.linalg.norm(right))\n",
    "\n",
    "for i, left in enumerate(sentences):\n",
    "    for j, right in enumerate(sentences):\n",
    "        if j < i:\n",
    "            continue\n",
    "        similarity = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        print(f'{similarity:.4f}: {left} <-> {right}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
